"""
AI Nexus - Skills Assessment Engine
Adaptive evaluation system to quantify AI proficiency
"""
import streamlit as st
import time
from utils.helpers import calculate_ai_score, save_to_local_storage

# Assessment Database
ASSESSMENT_QUESTIONS = [
    {
        "id": 1,
        "category": "Concepts",
        "question": "What is the primary difference between 'Fine-tuning' and 'RAG'?",
        "options": [
            "Fine-tuning changes model weights; RAG provides external context.",
            "RAG changes model weights; Fine-tuning provides external context.",
            "They are identical synonyms for training.",
            "Fine-tuning is only for images; RAG is for text."
        ],
        "correct": 0,
        "explanation": "Fine-tuning updates the neural network's weights to learn new patterns, while RAG (Retrieval Augmented Generation) injects relevant data into the prompt context window without retraining."
    },
    {
        "id": 2,
        "category": "Engineering",
        "question": "In a Vector Database, what does 'Cosine Similarity' measure?",
        "options": [
            "The file size of two documents.",
            "The semantic closeness between two embedding vectors.",
            "The number of shared words between sentences.",
            "The speed of database retrieval."
        ],
        "correct": 1,
        "explanation": "Cosine similarity measures the cosine of the angle between two vectors, effectively quantifying how semantically similar their represented content is."
    },
    {
        "id": 3,
        "category": "Prompting",
        "question": "Which technique involves providing the model with intermediate reasoning steps?",
        "options": [
            "Zero-shot Prompting",
            "Chain of Thought (CoT)",
            "Retrieval Augmented Generation",
            "Temperature Scaling"
        ],
        "correct": 1,
        "explanation": "Chain of Thought (CoT) encourages the model to 'think aloud' by breaking down complex problems into intermediate steps before giving the final answer."
    },
    {
        "id": 4,
        "category": "Architecture",
        "question": "What is a 'System Prompt' (or System Message)?",
        "options": [
            "The first message from the user.",
            "An error message generated by the server.",
            "A high-level instruction setting the AI's behavior, persona, and constraints.",
            "The output generated by the model."
        ],
        "correct": 2,
        "explanation": "The System Prompt is the initial instruction set (hidden from the end-user) that defines the AI's role, tone, and operational boundaries."
    },
    {
        "id": 5,
        "category": "Agents",
        "question": "In the ReAct pattern for AI Agents, what do the steps stand for?",
        "options": [
            "Read, Act",
            "Reason, Act",
            "Repeat, Acknowledge",
            "Review, Accept"
        ],
        "correct": 1,
        "explanation": "ReAct stands for 'Reason' (thinking about what to do) and 'Act' (using a tool or taking action), allowing agents to solve dynamic problems."
    },
    {
        "id": 6,
        "category": "Safety",
        "question": "What is a 'Hallucination' in the context of LLMs?",
        "options": [
            "A visual glitch in the UI.",
            "When the model generates plausible-sounding but factually incorrect information.",
            "A type of data augmentation technique.",
            "A memory optimization strategy."
        ],
        "correct": 1,
        "explanation": "Hallucinations occur when AI models confidently generate content that sounds correct but is factually wrong or made up. This is a key reliability concern."
    },
    {
        "id": 7,
        "category": "Parameters",
        "question": "What does the 'Temperature' parameter control in LLM generation?",
        "options": [
            "The speed of token generation.",
            "The randomness/creativity of the output.",
            "The maximum response length.",
            "The cost of the API call."
        ],
        "correct": 1,
        "explanation": "Temperature controls the randomness of outputs. Low values (0.0-0.3) produce deterministic, focused responses; high values (0.7-1.0) produce more creative, varied outputs."
    },
    {
        "id": 8,
        "category": "Fundamentals",
        "question": "What is 'Tokenization' in NLP?",
        "options": [
            "Converting text into numerical tokens the model can process.",
            "Encrypting user data for security.",
            "Creating API authentication tokens.",
            "Splitting a model across multiple GPUs."
        ],
        "correct": 0,
        "explanation": "Tokenization breaks text into smaller units (tokens) that the model can understand. These can be words, subwords, or characters depending on the tokenizer."
    },
    {
        "id": 9,
        "category": "Architecture",
        "question": "What is the 'Context Window' of an LLM?",
        "options": [
            "The UI panel where chat history is displayed.",
            "The maximum number of tokens the model can process in a single request.",
            "The time window for caching responses.",
            "The browser tab where the model runs."
        ],
        "correct": 1,
        "explanation": "The context window is the maximum amount of text (measured in tokens) that a model can 'see' and process at once. Larger windows allow for longer conversations and documents."
    },
    {
        "id": 10,
        "category": "Security",
        "question": "What is 'Prompt Injection'?",
        "options": [
            "A technique to speed up model inference.",
            "An attack where malicious input manipulates the AI's behavior.",
            "A method to fine-tune prompts automatically.",
            "A way to compress prompts to save tokens."
        ],
        "correct": 1,
        "explanation": "Prompt injection is a security vulnerability where attackers embed hidden instructions in user input to override the system prompt and manipulate the AI's behavior."
    }
]

def render():
    """Render the assessment page"""
    st.markdown("<h2>‚ö° AI Skills Assessment</h2>", unsafe_allow_html=True)
    
    # Initialize session state for quiz
    if 'assessment_state' not in st.session_state:
        st.session_state.assessment_state = {
            'started': False,
            'current_q': 0,
            'score': 0,
            'answers': {},
            'complete': False
        }
    
    state = st.session_state.assessment_state
    
    if not state['started']:
        render_intro(state)
    elif not state['complete']:
        render_quiz(state)
    else:
        render_results(state)

def render_intro(state):
    """Render the intro screen"""
    col1, col2 = st.columns([2, 1])
    with col1:
        st.markdown("""
        ### Ready to verify your skills?
        
        This assessment covers core AI Engineering concepts including:
        - **Generative AI Architecture** (RAG, Fine-tuning, Context Windows)
        - **Prompt Engineering** (CoT, System Prompts, Temperature)
        - **Vector Search & Embeddings**
        - **Agentic Patterns & Security**
        
        **Format:**
        - 10 High-Impact Questions
        - Instant Feedback
        - Profile Badge & Score Update
        """)
        
        st.markdown("<br>", unsafe_allow_html=True)
        if st.button("üöÄ Start Assessment", type="primary"):
            state['started'] = True
            st.rerun()
            
    with col2:
        st.markdown("""
            <div class="glass-card" style="text-align: center; padding: 2rem;">
                <div style="font-size: 3rem;">üìù</div>
                <div style="font-weight: 700; margin-top: 1rem;">10 Questions</div>
                <div style="color: #64748B;">Multiple Choice</div>
            </div>
        """, unsafe_allow_html=True)

def render_quiz(state):
    """Render the active quiz interface"""
    q_idx = state['current_q']
    question = ASSESSMENT_QUESTIONS[q_idx]
    
    # Progress bar
    progress = (q_idx + 1) / len(ASSESSMENT_QUESTIONS)
    st.progress(progress)
    st.caption(f"Question {q_idx + 1} of {len(ASSESSMENT_QUESTIONS)}")
    
    st.markdown(f"### {question['question']}")
    
    # Options
    response = st.radio(
        "Select your answer:",
        question['options'],
        key=f"q_{q_idx}",
        index=None
    )
    
    st.markdown("<br>", unsafe_allow_html=True)
    
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("Submit Answer", type="primary", disabled=response is None):
            # Check answer
            is_correct = question['options'].index(response) == question['correct']
            
            # Save result
            state['answers'][q_idx] = {
                'correct': is_correct,
                'selected': response,
                'explanation': question['explanation']
            }
            
            if is_correct:
                state['score'] += 1
                st.toast("‚úÖ Correct!", icon="‚úÖ")
            else:
                st.toast("‚ùå Incorrect", icon="‚ùå")
            
            # Move next or finish
            if q_idx + 1 < len(ASSESSMENT_QUESTIONS):
                state['current_q'] += 1
                # Small delay for UX
                time.sleep(0.5)
                st.rerun()
            else:
                state['complete'] = True
                st.rerun()

def render_results(state):
    """Render the results screen"""
    score_pct = (state['score'] / len(ASSESSMENT_QUESTIONS)) * 100
    
    st.balloons()
    
    st.markdown(f"""
        <div style="text-align: center; padding: 2rem;">
            <h1>Assessment Complete!</h1>
            <div style="font-size: 4rem; font-weight: 800; background: var(--prism-gradient); -webkit-background-clip: text; -webkit-text-fill-color: transparent;">{state['score']}/{len(ASSESSMENT_QUESTIONS)}</div>
            <p style="font-size: 1.2rem; color: #64748B;">You scored {score_pct}%</p>
        </div>
    """, unsafe_allow_html=True)
    
    # Analysis
    st.markdown("### üîç Performance Breakdown")
    
    for i, q in enumerate(ASSESSMENT_QUESTIONS):
        ans = state['answers'].get(i)
        if not ans: continue
        
        color = "#10B981" if ans['correct'] else "#EF4444"
        icon = "‚úÖ" if ans['correct'] else "‚ùå"
        
        with st.expander(f"{icon} Question {i+1}: {q['category']}", expanded=False):
            st.markdown(f"**Q:** {q['question']}")
            st.markdown(f"**Your Answer:** {ans['selected']}")
            if not ans['correct']:
                correct_txt = q['options'][q['correct']]
                st.markdown(f"**Correct Answer:** {correct_txt}")
            st.info(f"üí° **Insight:** {q['explanation']}")
            
    # Update Profile Action
    st.markdown("<br>", unsafe_allow_html=True)
    
    if st.button("üíæ Save Score to Profile", type="primary", use_container_width=True):
        if st.session_state.get('user_profile'):
            # Weighted score update (Assessment is worth a lot)
            new_ai_score = max(st.session_state.user_profile.get('ai_score', 0), int(score_pct))
            st.session_state.user_profile['ai_score'] = new_ai_score
            st.success(f"Profile updated! Your new AI Score is {new_ai_score}")
            
            # Reset and go back
            st.session_state.pop('assessment_state')
            st.session_state.current_page = "dashboard"
            time.sleep(1)
            st.rerun()
        else:
            st.error("Please create a profile first!")
